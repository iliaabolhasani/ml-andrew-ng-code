---
title: "معادله نرمال"
date: 2020-09-10T11:44:59+04:30
draft: false
weight : 60
---

### معادله نرمال

الگوریتم گرادیان کاهشی روشی بود برای مینیمم کردن
تابع $J$ ، اما روش دومی نیز وجود دارد که بدون داشتن
حلقه تکرار این کار را انجام بدهد که **<span class="top-dict" data-tipso="normal equation">معادله نرمال</span>** نام
دارد.

فرض کنید که تابع هزینه درجه دو ای مثل این داریم:
$$ J(\theta) = a\theta^2 + b\theta + c $$
$$ \frac{\partial} {\partial x} J(\theta)  \overset{\underset{\mathrm{set}}{}}{=}  0 $$

که برای مینیمم کردن این تابع درجه دو مشتق آن را
می‌گیریم و برابر با 0 قرار می‌دهیم، که این به ما
اجازه می‌دهد که مقدار $\theta$ را برای مینیمم کردن تابع 
پیدا کنیم.

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image80.png?width=13pc)

اما مسئله ای که برای ما جالب است $\theta$ یک عدد حقیقی
نیست، بلکه یک بردار در ابعدا 1+n  است:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image81.png?width=33pc)

برای محاسبه اینکه چطور تابع هزینه را مینیمم کنیم باید
مشتق جزئی تابع $J$ را برای هر کدام از تتا ها بگیریم و برابر 
با 0 قرار دهیم، بعد از محاسبه همه معادله ها مقدار 
تتا ای که تابع $J$  مینیمم می‌شود را به دست می‌آوریم.


اگر <span class="top-dict" data-tipso="training set">مجموعه آموزشی</span> به این شکل داشته باشیم:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image82.png?width=33pc)
![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image58.png?width=25pc)

معادله نرمال ما برای محاسبه تتا به این صورت خواهد
بود:

$$ \theta = (X^T X)^{-1} X^T y $$

با استفاده از معادله نرمال نیازی به <span class="top-dict" data-tipso="Feature Scaling">مقیاس بندی ویژگی</span>  نداریم، و برای مقایسه گرادیان کاهشی و معادله نرمال:


| گرادیان کاهشی | معادله نرمال |
| ------: | -----------: |
| به تنظیم پارامتر آلفا نیاز دارد | به تنظیم پارامتر آلفا نیاز ندارد |
| به تکرار نیاز دارد | به تکرار نیاز ندارد |
| مرتبه زمانی اش $O(kn)^3 $ است |   مرتبه زمانی اش $O(n)^3$ است  |
| برای تعداد ویژگی های زیاد  خوب کار می‌کند | برای تعداد ویژگی های زیاد کند است |