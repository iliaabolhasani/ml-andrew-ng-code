---
title: "تحلیل اجزای اصلی الگوریتم"
date: 2020-10-26T09:17:16+03:30
draft: false
weight: 80
---

قبل از استفاده از PCA ، یک مرحله پیش پردازش داده وجود دارد که باید انجام دهیم:

### پیش پردازش داده

- گرفتن مجموعه آموزشی: $x^{(1)}, x^{(2)}, ... , x^{(m)}$
- پیش پردازش (feature scaling/mean normalization):
$\mu _j = \frac{1}{m} \sum _ {i=1} ^ m x ^ {(i)} _j  $

- جایگزین کردن هر $x ^ {(i)} _j $ با $x ^ {(i)} _j - \mu _j$
- اگر ویژگی های مختلف در مقیاس های متفاوت باشند (مثلا $= x_1$ اندازه خانه، $ = x_2$ تعداد اتاق خواب ها)،
ویژگی ها را برای داشتن دامنه ای قابل مقایسه از مقادیر تغییر مقیاس دهید.



در بالا ، ابتدا میانگین هر ویژگی را از ویژگی اصلی کم می‌کنیم. سپس تمام ویژگی ها را مقیاس بندی می کنیم:
$$
x ^ {(i)} _j = \frac{x ^ {(i)} _j - \mu _j}{s_j}
$$

می‌توان به طور خاص کاهش از ۲ بعدی به ۱ بعدی داده را به این صورت تعریف کرد:

$$
\sum  = \frac{1}{m} \sum _ {i=1} ^ m (x ^ {(i)}) (x ^ {(i)}) ^ T
$$

مقادیر z، همه اعداد حقیقی هستند و پیش بینی ویژگی های ما بر روی $u ^{(1)}$ هستند.

بنابراین PCA دو وظیفه دارد: 
کشف کردن $u^{(1)}, u^{(2)}, ..., u^{(k)}$ ها، 
و پیدا کردن $z_1, z_2, ... , z_m$

اثبات ریاضی برای روش زیر پیچیده و فراتر از محدوده این دوره است.

**1. محاسبه ماتریس کو واریانس**
$$
\sum  = \frac{1}{m} \sum _ {i=1} ^ m (x ^ {(i)}) (x ^ {(i)}) ^ T
$$

این را می‌توان در اوکتاو به صورت زیر برداری کرد:

<div align="left">

```
Sigma = (1/m) * X' * X
```

</div>

ما ماتریس کو واریانس را با یک سیگما بزرگ نشان می دهیم.


توجه داشته باشید که $x ^ {(i)}$ یک بردار $n \times 1$ است،
$(x ^ {(i)})^T$ یک بردار $1 \times n$ است و 
X یک ماتریس $m \times n$ است که نتیجه ضرب آن ها
ماتریسی $n \times n$ است، که ابعاد $\sum$ است.



**2. محاسبه بردار های ویژه ماتریس کو واریانس $\sum$**

<div align="left">

```
[U,S,V] = svd(Sigma)
```

</div>

svd  مخفف کلمه singular value decomposition است که یک تابع توکار در اوکتاو است.

در واقع آن چیزی که به عنوان خروجی از svd می‌خواهیم،
$U$ یا ماتریس کو واریانس سیگما است:

$$
U \in \mathbb{R} ^{n \times n}
$$

$$
U = u ^ {(1)}, ..., u ^ {(n)}
$$


**3. k ستون اول ماتریس U را گرفته و z را محاسبه کنید**

ما k ستون اول U به متغیری به اسم $U _ {reduce}$ اختصاص می‌دهیم،
که یک ماتریس $n \times k$ خواهد شد.
و $z$ را به این صورت محاسبه می‌کنیم:

$$
z^{(i)} = Ureduce ^ T . x ^ {(i)}
$$
$UreduceZ^ T$ ابعادی برابر با $k \times n$ خواهد داشت.
در حالی که ابعاد $x ^ {(i)}$، $n \times 1$ خواهد بود.
نتیجه $Ureduce ^ T . x ^ {(i)}$ ابعادی برابر با $k \times 1$ خواهد داشت.

به طور خلاصه، کل الگوریتم موجود در اکتاو به این صورت خواهد بود:

<div align="left">

```
Sigma = (1/m) * X' * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
```

</div>