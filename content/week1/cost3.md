---
title: "تابع هزینه قسمت سوم"
date: 2020-09-06T16:31:03+04:30
draft: false
weight : 70
---

قسمت قبل دیدیم که با داشتن فقط یک پارامتر برای
تابع <span class="top-dict" data-tipso="hypothesis">فرضیه</span>
 نمودار <span class="top-dict" data-tipso="cost function">تابع هزینه</span>
 یا همان $J$  به صورت
سهمی بود.
اگر دو پارامتر داشته باشیم باز هم به صورت سهمی
است، اما سه بعدی و بسته به داده ما ممکن است به
شکل زیر باشد:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image22.png?width=25pc)

اما ما برای نمایش این تابع از شکل سه بعدی استفاده
نمی‌کنیم‌، بلکه از 
**<span class="top-dict" data-tipso="contour plot">نمودار های کانتور</span>**
استفاده می‌کنیم!

در این نمودار ها هر یک از بیضی ها نشان دهنده
مجموعه ای از نقاط است که مقادیر یکسانی در $J$
بر حسب $\theta_0$ و $\theta_1$ های مختلف دارند.

**مثالی از یک نمودار کانتور:**

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image25.png?width=20pc)

مثلا نقطه قرمز روی نمودار کانتور سمت راست برابر
است با: $ \theta_1 = -0.15, \theta_0 = 800 $

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image18.png?width=35pc)

اما همینطور که می‌بینیم خط حاصل از تابع فرضیه
تناسب خوبی با داده های ما ندارد!
به این خاطر که نقطه ما از مینیمم که کوچکترین 
بیضی است خیلی دور است!

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image19.png?width=35pc)

در این نقطه جدید هم کاملا مینیمم نیست اما خیلی
بهتر از قبلی است.
باز هم خط حاصل از تابع فرضیه بر حسب مقادیر 
انتخابی برای دو پارامتر مسئله با داده های ما 
متناسب نیست ...

در واقع ما به الگوریتمی نیاز داریم که برای ما مقادیر
$\theta_0$ و $\theta_1$  را در حالتی که تابع $J$  مینیمم است
بیابد.

**که پاسخ ما  <span class="top-dict" data-tipso="gradient descent">گرادیان کاهشی</span> است !**

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image28.png?width=35pc)
