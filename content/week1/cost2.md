---
title: "تابع هزینه قسمت دوم"
date: 2020-09-06T14:26:42+04:30
draft: false
weight : 60
---


تا اینجا به طور خلاصه تمام چیزی که از تابع هزینه
می‌دانیم در زیر آمده است:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image13.png?width=20pc)


اما اجازه بدید برای ساده سازی تابع فرضیه را تنها با
یک پارامتر به این شکل در نظر بگیریم:
$ h_\theta(x) = \theta_1x $
و سه مقدار مختلف $0$، $5.0 $ و $1$ رو حساب کنیم ...

مثلا برای مقدار تتا برابر با $1$ محاسبات زیر را خواهیم داشت:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image15.png?width=30pc)

$$ {\color{Red} J(\theta_1) =  \frac{1}{2m} \sum_{i=1}^{m} (\theta_1x - y_i)^2 \Rightarrow \frac{1}{2m} (0^2 + 0^2 + 0^2) = 0 } $$
به همین صورت برای دو مقدار دیگر داریم:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image16.png?width=30pc)

$$ {\color{Blue}  J(0.5) =  \frac{1}{2m} [ (0.5 - 1)^2 + (1-2)^2 + (1.5 -3)^2] \Rightarrow 0.58 }  $$
$$ {\color{Green}  J(0) =  \frac{1}{2m} ( 1^2 + 2^2 + 3^2 +) \Rightarrow 2.3  }$$

و اگر به همین ترتیب برای مقادیر دیگر رسم کنیم:

![دوره یادگیری ماشین دانشگاه استنفورد به فارسی](../images/image17.png?width=30pc)

متوجه می‌شویم که به ازای هر مقدار تتا به یک تابع
فرضیه متفاوت و یک مقدار متفاوت برای تابع $J$ می‌رسیم
و همینطور که می‌بینیم در نقطه $1$ در مینیمم ترین
حالت ممکن هستیم و این همان هدف ما است!