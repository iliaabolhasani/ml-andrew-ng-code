---
title: "ارائه مدل قسمت اول"
date: 2020-09-17T11:13:34+04:30
draft: false
weight: 30
---

**چگونه یک تابع فرضیه را با استفاده از شبکه های عصبی نشان خواهیم داد؟**

شبکه های عصبی به عنوان روشی برای شبیه سازی نورون ها یا شبکه ای از نورون های در مغز ساخته شده اند.

یک نورون در مغز به این شکل است:

![image6.png](../images/image6.png?width=30pc)

که به طور کلی از سه بخش قابل توجه زیر ساخته شده است:
- بدنه سلول 
- بخش ورودی <span class="top-dict" data-tipso="dendrites">دنریت</span>
- بخش خروجی <span class="top-dict" data-tipso="axon">اکسون</span>

**به طور ساده می‌توانیم بگوییم که:**

نورون ها اساسا واحد های محاسباتی هستند که از طریق رشته سیم های دنریت پالس های الکتریکی به اسم  <span class="top-dict" data-tipso="spikes">اسپایک</span> را ورودی می‌گیرند،
و آن ها را به خروجی یا همان اکسون هدایت می‌کنند.
بنابراین متوجه می‌شویم که نورون ها توسط پالس های الکتریکی با هم ارتباط برقرار می‌کنند.

در شبکه های عصبی مصنوعی، نورون ها واحد هایی لجستیکی هستند:

![image7.png](../images/image7.png?width=30pc)

در مدل ما دنریت ها شبیه به ویژگی ها ورودی $x_1 ... x_n$ هستند، 
و خروجی ما تابع فرضیه است.

در این مدل گره ورودی $x_0$ **واحد <span class="top-dict" data-tipso="bias">بایاس</span>** یا  نام دارد که همیشه برابر با مقدار ۱ است.
در شبکه های عصبی ما از تابع لجستیکی که در طبقه بندی داشتیم استفاده می‌کنیم:
$$
\frac{1}{1+e^{- \theta^T x}}
$$
اگر چه که در شبکه های عصبی گاهی اوقات آن را **<span class="top-dict" data-tipso="activation function">تابع فعال سازی</span> <span class="top-dict" data-tipso="sigmoid">سیگموئید</span>** صدا می‌زنیم،
بردار $\theta$ نیز weights یا وزن های مدل نامیده می‌شوند.

به طور ساده می‌توانیم به این شکل نمایش دهیم:


![image8.png](../images/image8.png?width=13pc)

گره های ورودی یا لایه اول داخل لایه دوم می‌شوند، و خروجی هم تابع فرضیه است.
لایه اول را <span class="top-dict" data-tipso="input layer">لایه ورودی</span> می‌نامیم،
و لایه دوم را هم <span class="top-dict" data-tipso="output layer">لایه خروجی</span> می‌نامیم،
که تابع فرضیه را به عنوان خروجی نتیجه می‌دهد.

ما می‌توانیم لایه های میانی از گره ها داشته باشیم که بین لایه ورودی و خروجی قرار می‌گیرند، که
به آن ها لایه های پنهان می‌گوییم.

ما <span class="top-dict" data-tipso="node">گره</span> های لایه های میانی یا پنهان را به صورت
$a_0 ^2 ... a_n ^2$ نام گذاری می‌کنیم،
و به آن ها <span class="top-dict" data-tipso="activation units">واحد های فعال سازی</span> می‌گوییم.

$$
\begin{align*}& a_i^{(j)} = \text{"activation" of unit $i$ in layer $j$} \newline& \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\end{align*}
$$


به طور مثال اگر فقط یک <span class="top-dict" data-tipso="hidden layer">لایه پنهان</span> داشته باشیم به این شکل می‌شود:

![image9.png](../images/image9.png?width=13pc)

مقدار هر گره فعال ساز به صورت زیر به دست می‌آید:

$$
\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}
$$

![image10.png](../images/image10.png?width=40pc)


این به ما می‌گوید که گره های فعال ساز مان را با استفاده از یک ماتریس
$3 \times 4$ از پارامتر ها محاسبه می‌کنیم.
به این ترتیب که ما هر ردیف (row) از پارامتر ها را به ورودی های خود
اعمال می‌کنیم، تا مقدار یک گره فعال ساز را بدست آوریم.

![image12.png](../images/image12.png?width=40pc)
![image11.png](../images/image11.png?width=40pc)

و خروجی فرضیه ما تابعی لجستیکی است که روی مجموع مقادیر گره های فعال ساز ما اعمال می‌شود،
که در ماتریسی از پارامتر های دیگری ($\Theta^{(2)}$) ضرب می‌شود.

**محاسبه $a_1 ^{(2)}$ :**
![image13.png](../images/image13.png?width=30pc)

**محاسبه $a_2 ^{(2)}$ :**
![image14.png](../images/image14.png?width=30pc)

**محاسبه $a_3 ^{(2)}$ :**

[تصحیح: ردیف سوم از ماتریس $\Theta^T X$ باید انتخاب شود]
![image15.png](../images/image15.png?width=30pc)

**محاسبه** $a_1 ^{(3)}$ **یا همان** $h_\Theta(x)$:
![image16.png](../images/image16.png?width=30pc)


{{% notice note %}}
$\Theta^{(2)}$
شامل وزن های گره های لایه دوم است.
{{% /notice %}}

به طور کلی هر لابه ماتریس وزن خود را دارد که به شکل
$ \Theta^{(j)} $ نام گذاری می‌شود.

**ابعاد این ماتریس های وزنی به شکل زیر محاسبه می‌شوند:**

اگر شبکه ما شامل $s_j$ واحد در هر لایه $j$ است،
و $s_{j+1}$ واحد داخل لایه $j+1$ ام است،
سپس ابعاد ماتریس $\Theta^{(j)}$ برابر با
$ s_{j+1} \times (s_j + 1) $ خواهد بود.

{{% notice note %}}
1+ آورده شده به خاطر
گره بایاس است.
گره بایاس در گره های خروجی شامل نمی‌شود، اما در گره های ورودی وجود دارد.
{{% /notice %}}

**مثال:**

اگر لایه اول ۲ گره ورودی،
و لایه دوم ۴ گره فعال ساز داشته باشد،
ابعاد $\Theta^{(1)}$ به صورت 
$4 \times 3$ خواهد بود،
به طوری که:

$$ s_j = 2,  s_{j+1} = 4 $$

$$ s_{j+1} \times (s_j + 1) = 4 \times 3 $$


